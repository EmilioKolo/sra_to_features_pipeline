#!/usr/bin/env python3


"""
Functions to be used for the main pipeline.
"""

from collections import Counter, defaultdict
from scripts.log_scripts import *
import gzip
import json
import os
import pandas as pd
import random
import requests
import statistics
import time
import tempfile


### Modular functions ###

def download_sra(dict_var:dict) -> dict:
    """
    Downloads the corresponding fastq for a given SRA ID.

    Takes the dictionary of variables generated by 
    arg_handler.get_variables()
    """
    # Define sra_id
    sra_id = dict_var['sra_id']
    # Obtain SRA data
    sra_data = sra_id_data(sra_id, log_file=dict_var['log_print'])
    cont = 0
    while sra_data is None:
        # Add random wait time
        time.sleep(random.randint(15, 30))
        w = f"Retrying to get SRA data for {sra_id}"
        w += f" (attempt {cont+1})"
        log_print(
            w,
            level='warn',
            log_file=dict_var['log_print']
        )
        # Increment the counter
        cont+=1
        # Retry getting SRA data
        sra_data = sra_id_data(sra_id, log_file=dict_var['log_print'])
        if cont>5:
            er = "Could not retrieve SRA data for ID: "
            er += str(sra_id)
            er += f' after {cont} attempts'
            raise TimeoutError(er)
    
    # Check for files associated to the SRA ID
    l_ftp = sra_data['fastq_ftp'].split(';')
    if len(l_ftp)==1: # Single end
        dict_var['l_fastq'] = ['']
        dict_var['l_fastq'][0] = l_ftp[0].split('/')[-1]
    elif len(l_ftp)==2: # Paired end
        dict_var['l_fastq'] = ['', '']
        dict_var['l_fastq'][0] = l_ftp[0].split('/')[-1]
        dict_var['l_fastq'][1] = l_ftp[1].split('/')[-1]
    else:
        er = f'More than two elements in l_ftp.\n{l_ftp}'
        raise ValueError(er)
    
    # Define a list of fastq files with their directory included
    l_fastq_full = []
    for i in dict_var['l_fastq']:
        l_fastq_full.append(os.path.join(dict_var['fastq_dir'], i))
    dict_var['l_fastq_full'] = l_fastq_full

    ### Display
    t = 'Downloading and extracting FASTQ files for'
    t += f' {sra_id} using fastq-dump...'
    log_print(
        t,
        level='info',
        log_file=dict_var['log_print']
    )
    ###

    # Download fastq files
    sra_to_fastq(
        sra_id,
        l_fastq=dict_var['l_fastq_full'],
        output_dir=dict_var['fastq_dir'],
        log_file=dict_var['log_print'],
        log_scr=dict_var['log_scripts']
    )

    # Visualize file sizes
    t = f'Reads downloaded and extracted to:'
    for i in dict_var['l_fastq_full']:
        t += f'\n{i}'
    log_print(
        t,
        level='info',
        log_file=dict_var['log_print']
    )
    log_print(
        t,
        level='info',
        log_file=dict_var['log_print']
    )
    l = 'du -h'
    for i in dict_var['l_fastq_full']:
        l += ' ' + i
    log_code(
        l,
        log_file=dict_var['log_scripts']
    )
    os.system(l)
    return dict_var

def align_to_reference(dict_var:dict) -> dict:
    """
    Aligns fastq files to a reference sequence and obtains BAM files.

    Takes the dictionary of variables generated by 
    arg_handler.get_variables() with added variables
    from download_sra()
    """
    # Align fastq files to the reference genome
    sam_file = bwa_align(
        dict_var['sra_id'],
        dict_var['l_fastq_full'],
        ref_genome=dict_var['fasta_ref'],
        output_dir=dict_var['sam_bam_dir'],
        log_scr=dict_var['log_scripts'],
        threads=dict_var['THREADS']
    )
    log_print(
        f"Alignment to SAM file complete: {sam_file}",
        level='info',
        log_file=dict_var['log_print']
    )
    # Check if the SAM file was created and has content
    log_print(
        "Checking SAM file content...",
        level='info',
        log_file=dict_var['log_print']
    )
    l = f'head -n 10 {sam_file}'
    log_code(
        l,
        log_file=dict_var['log_scripts']
    )
    os.system(l)
    # Check file size
    l = f'ls -lh {sam_file}'
    log_code(
        l,
        log_file=dict_var['log_scripts']
    )
    os.system(l)

    # Transform the sam file into a bam file
    bam_file = sam_to_bam(
        dict_var['sra_id'],
        sam_file,
        output_dir=dict_var['sam_bam_dir'],
        log_scr=dict_var['log_scripts']
    )
    log_print(
        f"SAM to BAM conversion complete: {sam_file}",
        level='info',
        log_file=dict_var['log_print']
    )
    # Check file size
    l = f'ls -lh {bam_file}'
    log_code(
        l,
        log_file=dict_var['log_scripts']
    )
    os.system(l)
    # Remove the sam file
    os.remove(sam_file)
    # Index and sort the bam file
    sorted_bam = sort_index_bam(
        dict_var['sra_id'],
        bam_file,
        output_dir=dict_var['sam_bam_dir'],
        log_file=dict_var['log_print'],
        log_scr=dict_var['log_scripts']
    )

    # Check sorted file size
    l = f'ls -lh {sorted_bam}'
    log_code(
        l,
        log_file=dict_var['log_scripts']
    )
    os.system(l)
    # Check index file size
    l = f'ls -lh {sorted_bam}.bai'
    log_code(
        l,
        log_file=dict_var['log_scripts']
    )
    os.system(l)
    # Generate alignment statistics
    log_print(
        "Alignment statistics...",
        level='info',
        log_file=dict_var['log_print']
    )
    l = f'samtools flagstat {sorted_bam}'
    log_code(
        l,
        log_file=dict_var['log_scripts']
    )
    os.system(l)

    # Add new variables to dict_var
    dict_var['sorted_bam'] = sorted_bam

    return dict_var

def variant_call_and_analysis(dict_var:dict) -> dict:
    """
    Calls variants from a sorted and indexed BAM file and analyzes the
    variants with snpEff.

    Takes the dictionary of variables generated by 
    arg_handler.get_variables() with added variables
    from download_sra() and align_to_reference()
    """
    # Perform variant calling
    vcf_file = variant_call_mpileup(
        dict_var['sra_id'],
        dict_var['sorted_bam'],
        ref_genome=dict_var['fasta_ref'],
        output_dir=dict_var['vcf_dir'],
        log_file=dict_var['log_print'],
        log_scr=dict_var['log_scripts']
    )
    
    # Compress and index the created vcf file
    compressed_vcf = compress_index_vcf(
        dict_var['sra_id'],
        vcf_file,
        dict_var['OUTPUT_DIR'],
        log_file=dict_var['log_print'],
        log_scr=dict_var['log_scripts']
    )
    # Check compressed vcf stats
    log_print(
        "Variant calling statistics.",
        level='info',
        log_file=dict_var['log_print']
    )
    l = f'bcftools stats'
    l += f' {compressed_vcf}'
    l += f' > {compressed_vcf}.stats'
    log_code(
        l,
        log_file=dict_var['log_scripts']
    )
    os.system(l)

    # Analyze variants in VCF with snpeff
    compressed_snpeff_vcf = variant_analysis_snpeff(
        dict_var['sra_id'],
        compressed_vcf,
        dict_var['genome_name'],
        dict_var['snpeff_dir'],
        dict_var['OUTPUT_DIR'],
        log_file=dict_var['log_print'],
        log_scr=dict_var['log_scripts']
    )

    # Add relevant variables to dict_var
    dict_var['compressed_vcf'] = compressed_vcf
    dict_var['compressed_snpeff_vcf'] = compressed_snpeff_vcf
    return dict_var

def feature_generation(dict_var:dict) -> dict[str:float|int]:
    """
    Generates features from the different generated files.
    """
    # Initialize the dictionary of features
    dict_features = {}
    # Define an output folder for feature files
    feat_dir = os.path.join(dict_var['OUTPUT_DIR'], 'feature_files')
    # Make the directory if it does not exist
    try:
        os.mkdir(feat_dir)
    except FileExistsError:
        pass
    # Make the temporary folder for intermediate files if it does not exist
    try:
        os.mkdir(dict_var['tmp_sra'])
    except FileExistsError:
        pass

    # Get the features associated with fragment length
    if len(dict_var['l_fastq'])==2:
        log_print(
            'Calculating fragment length of reads',
            level='info',
            log_file=dict_var['log_print']
        )
        dict_features = ft_fragment_lengths(
            dict_var['sra_id'],
            dict_var['sorted_bam'],
            feat_dir,
            dict_features,
            log_file=dict_var['log_print'],
            log_scr=dict_var['log_scripts']
        )
    else:
        log_print(
            'Single-end reads. Fragment length not calculated.',
            level='warn',
            log_file=dict_var['log_print']
        )
        dict_features['fl_mean'] = 'NA'
        dict_features['fl_median'] = 'NA'
        dict_features['fl_stdv'] = 'NA'

    # Get the CNV features
    try:
        dict_features = ft_cnv_prediction(
            dict_var['sra_id'],
            dict_var['sorted_bam'],
            dict_var['compressed_snpeff_vcf'],
            dict_var['bin_size_cnv'],
            feat_dir,
            dict_features,
            log_file=dict_var['log_print'],
            log_scr=dict_var['log_scripts']
        )
    except Exception as e:
        log_print(
            f"Error in CNV prediction: {e}",
            level='error',
            log_file=dict_var['log_print']
        )

    # Get the dn/ds variant proportion features
    try:
        dict_features = ft_dn_ds(
            dict_var['compressed_snpeff_vcf'],
            dict_var['bed_genes'],
            dict_var['bed_variants'],
            dict_var['bed_intersect'],
            feat_dir,
            dict_features,
            log_file=dict_var['log_print'],
            log_scr=dict_var['log_scripts']
        )
    except Exception as e:
        log_print(
            f"Error in dn/ds calculation: {e}",
            level='error',
            log_file=dict_var['log_print']
        )

    # Get the GV per region features
    try:
        dict_features = ft_gv_per_region(
            dict_var['compressed_snpeff_vcf'],
            dict_var['gff_ref'],
            dict_var['bed_file'],
            dict_var['counts_file'],
            dict_var['genome_sizes'],
            dict_var['bin_size_gvs'],
            feat_dir,
            dict_features,
            log_file=dict_var['log_print'],
            log_scr=dict_var['log_scripts']
        )
    except Exception as e:
        log_print(
            f"Error in gv per gene region count: {e}",
            level='error',
            log_file=dict_var['log_print']
        )

    return dict_features

def save_features(dict_var:dict, dict_features:dict) -> None:
    """
    Save the features in dict_features using values from dict_var.
    """
    # Transform dict_features into a pandas DataFrame
    df_features = pd.DataFrame([dict_features])
    # Define output features file name
    features_file = os.path.join(
        dict_var['OUTPUT_DIR'],
        dict_var['sra_id']+'_features.csv'
    )
    log_print(
        f'Saving features to: {features_file}',
        level='info',
        log_file=dict_var['log_print']
    )
    try:
        df_features.transpose().to_csv(
            features_file,
            sep=';',
            header=[dict_var['sra_id']]
        )
    except:
        df_features.transpose().to_csv(
            features_file,
            sep=';',
            header=False
        )
    log_print(
        'Features loaded and saved.',
        level='info',
        log_file=dict_var['log_print']
    )

    log_print(
        'Changing ownership of output files...',
        level='info',
        log_file=dict_var['log_print']
    )
    # Change ownership of output files to the host user
    change_output_ownership(
        dict_var['OUTPUT_DIR'],
        log_file=dict_var['log_print']
    )

    return None

### Shorter functions ###

def bwa_align(
        sra_id:str,
        l_fastq:list[str],
        ref_genome:str,
        output_dir:str,
        log_scr:str,
        threads:int=2
    ) -> str:
    """
    Aligns the reads from a list of fastq files to a reference genome.
    Returns the path to the output SAM file.
    """
    # Make the output directory if it does not exist
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        pass
    # Define output
    output_sam = os.path.join(output_dir, sra_id+'.sam')
    # Initialize script
    l = f'bwa mem -M -t {threads}'
    # Define reference genome
    l += f' {ref_genome}'
    # Define fastq files
    for fastq in l_fastq:
        l += f' {fastq}'
    # Define output file
    l += f' > {output_sam}'
    # Run the script
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    return output_sam

def change_output_ownership(output_dir:str, log_file:str) -> None:
    """
    Changes the ownership of all files and directories in output_dir.
    Requires HOST_UID and HOST_GID environment variables to be set.
    """
    # Get the UID and GID from environment variables
    try:
        uid = int(os.environ["HOST_UID"])
        gid = int(os.environ["HOST_GID"])
    except KeyError as e:
        w = 'Permissions cannot be changed.'
        w += '\nTo do it manually, run:'
        w += f'\nsudo chmod 777 {output_dir}'
        log_print(
            w,
            level='warn',
            log_file=log_file
        )
        return None
    # Walk through the output directory
    for root, dirs, files in os.walk(output_dir):
        for name in dirs + files:
            path = os.path.join(root, name)
            try:
                os.chown(path, uid, gid)
                log_print(
                    f"Changed ownership of file {path}",
                    level='info',
                    log_file=log_file
                )
            except PermissionError as e:
                log_print(
                    f"Permission denied on {path}: {e}",
                    level='warn',
                    log_file=log_file
                )
            except FileNotFoundError:
                continue
    return None

def compress_index_vcf(
        sra_id:str,
        vcf_file:str,
        output_dir:str,
        log_file:str,
        log_scr:str
    ) -> str:
    """
    Compresses and indexes a given VCF file.
    Returns the path to the compressed and indexed VCF file.
    """
    # Make the output directory if it does not exist
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        pass
    # Define output files
    compressed_vcf = os.path.join(output_dir, sra_id+'.vcf.gz')
    # Compress VCF
    log_print(
        f"Compressing {vcf_file} with bgzip...",
        level='info',
        log_file=log_file
    )
    # Define bgzip script
    l = f'bgzip'
    # Define input file
    l += f' -c {vcf_file}'
    # Define output file
    l += f' > {compressed_vcf}'
    # Run the script
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    log_print(
        f"Compressed VCF: {compressed_vcf}",
        level='info',
        log_file=log_file
    )

    log_print(
        f"Indexing {compressed_vcf} with tabix...",
        level='info',
        log_file=log_file
    )
    # Define tabix script
    l = f'tabix -p vcf {compressed_vcf}'
    # Run the script
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    log_print(
        f"VCF index: {compressed_vcf}.tbi",
        level='info',
        log_file=log_file
    )
    return compressed_vcf

def create_counts(
        counts_file:str,
        regions:list[list[str,int,int,str]],
        vcf_compressed:str,
        log_scr:str
    ) -> None:
    """
    Create a counts file with number of variants per region.
    """
    # Clear intermediary output file before appending
    with open(counts_file, "w") as f:
        f.write("")
    # Load regions and counts into out_folder
    for chrom, start, end, name in regions:
        # Define region string
        region_str = f"{chrom}:{start}-{end}"
        # Add region name
        with open(counts_file, "a+") as f:
            f.write(f"{name}\t")
        # Use environ to define variables
        os.environ['region_str'] = region_str
        os.environ['vcf_file'] = vcf_compressed
        os.environ['tmp_output'] = counts_file
        # Use bcftools to check how many variables are in region_str
        l = 'bcftools view -r "$region_str"'
        l += ' "$vcf_file" | grep -vc "^#" >> "$tmp_output"'
        # No code log here, too slow
        os.system(l)
    return None

def extract_regions(bed_file:str) -> list[list[str,int,int,str]]:
    """
    Extracts regions of the genome from bed_file and formats them.
    Returns a list of regions with format chr, start, end, name.
    """
    # Get regions from bed_file
    regions = []
    with open(bed_file) as f:
        # Go through bed file
        for line in f:
            # Discard empty lines and commented lines
            if line.strip() and not line.startswith("#"):
                # Split the line
                fields = line.strip().split('\t')
                # chr, start and end should be in the first 3 fields
                chrom, start, end = fields[:3]
                # Fourth field can be region name (i.e. for genes)
                if len(fields) > 3:
                    name = fields[3]
                else:
                    name = f"{chrom}:{start}-{end}"
                # Add to region list
                regions.append((chrom, int(start), int(end), name))
    return regions

def ft_cnv_prediction(
        sra_id:str,
        bam_file:str,
        snpeff_vcf:str,
        bin_sizes:int,
        output_dir:str,
        feature_dict:dict[str:float|int],
        log_file:str,
        log_scr:str
    ) -> dict[str:float|int]:
    """
    Calculates the number of CNVs per chromosome given a BAM file.
    """
    # Make the output directory if it does not exist
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        pass

    log_print(
        'Starting CNV calling...',
        level='info',
        log_file=log_file
    )
    # Define the root file for CNVpytor
    ROOT_FILE = os.path.join(output_dir, f"{sra_id}.pytor")
    log_print(
        f"CNVpytor root file will be: {ROOT_FILE}",
        level='info',
        log_file=log_file
    )
    # Remove existing root file if it exists
    if os.path.exists(ROOT_FILE):
        os.remove(ROOT_FILE)
    
    # Check if input files exist
    bool_bam = os.path.exists(bam_file)
    bool_bai = os.path.exists(f"{bam_file}.bai")
    bool_crai = os.path.exists(f"{bam_file}.crai")
    if not bool_bam:
        log_print(
            f"BAM/CRAM file not found at {bam_file}",
            level='error',
            log_file=log_file
        )
        exit()
    if not (bool_bai or bool_crai):
        t = "BAM/CRAM index file not found."
        t += f" Please ensure '{bam_file}.bai' or '{bam_file}.crai'"
        t += " exists alongside the BAM/CRAM. CNVpytor might fail."
        log_print(
            t,
            level='warn',
            log_file=log_file
        )
    # Define if you have a VCF/GVCF
    use_baf = True
    bool_vcf = os.path.exists(snpeff_vcf)
    bool_tbi = os.path.exists(f"{snpeff_vcf}.tbi")
    bool_gz_tbi = os.path.exists(f"{snpeff_vcf}.gz.tbi")
    if not bool_vcf:
        t = f"VCF/GVCF file not found at {snpeff_vcf}."
        t += " BAF analysis will be skipped."
        log_print(
            t,
            level='warn',
            log_file=log_file
        )
        use_baf = False
    elif not (bool_tbi or bool_gz_tbi):
        t = "VCF/GVCF index file not found."
        t += f" Ensure '{snpeff_vcf}.tbi' or '{snpeff_vcf}.gz.tbi'"
        t += " exists alongside the VCF/GVCF."
        t += " BAF analysis might fail or be inaccurate."
        log_print(
            t,
            level='warn',
            log_file=log_file
        )
    
    # Process Read Depth (RD) Data
    log_print(
        "Processing Read Depth (RD) data...",
        level='info',
        log_file=log_file
    )
    l = f'cnvpytor -root {ROOT_FILE} -rd {bam_file}'
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    log_print(
        "Read Depth processing complete.",
        level='info',
        log_file=log_file
    )

    # Process BAF (BAF) Data
    if use_baf:
        log_print(
            "Processing B-allele Frequency (BAF) data...",
            level='info',
            log_file=log_file
        )
        # First, add SNPs from VCF to the root file
        l = f'cnvpytor -root {ROOT_FILE}'
        l += f' -snp {snpeff_vcf} -sample {sra_id}'
        log_code(
            l,
            log_file=log_scr
        )
        os.system(l)
        # Then, perform BAF analysis with specified bin sizes
        l = f'cnvpytor -root {ROOT_FILE} -baf {bin_sizes}'
        log_code(
            l,
            log_file=log_scr
        )
        os.system(l)
        t = "B-allele Frequency processing complete."
        log_print(
            t,
            level='info',
            log_file=log_file
        )
    else:
        t = "BAF analysis skipped as specified"
        t += " or due to missing VCF/index."
        log_print(
            t,
            level='info',
            log_file=log_file
        )
    # Create Histograms and Partitioning
    t = "Generating histograms and partitioning data..."
    log_print(
        t,
        level='info',
        log_file=log_file
    )
    # Create histograms for RD and BAF (if used)
    l = f'cnvpytor -root {ROOT_FILE} -his {bin_sizes} --verbose debug'
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    # Partition data for CNV calling
    l = f'cnvpytor -root {ROOT_FILE}'
    l += f' -partition {bin_sizes} --verbose debug'
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    log_print(
        "Histograms and partitioning complete.",
        level='info',
        log_file=log_file
    )
    # Call CNVs
    cnv_call_file = f'{output_dir}/cnv_calls_{bin_sizes}.txt'
    l = f'cnvpytor -root {ROOT_FILE} -call {bin_sizes} > {cnv_call_file}'
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    log_print(
        "CNV calling complete.",
        level='info',
        log_file=log_file
    )

    # Read CNVpytor output with python 
    calls = []
    with open(cnv_call_file, 'r') as f:
        for line in f.readlines():
            calls.append(line.rstrip('\n').split('\t'))
    # Count CNVs per chromosome
    cnv_counts = Counter()
    for call in calls:
        chrom = call[1].split(':')[0]
        cnv_counts[chrom] += 1
    # Save result
    for chrom, count in sorted(cnv_counts.items()):
        if str(chrom).startswith('chr'):
            log_print(
                f"{chrom}: {count} CNVs",
                level='info',
                log_file=log_file
            )
            key = f'{chrom}_cnv_count'
        else:
            log_print(
                f"chr{chrom}: {count} CNVs",
                level='info',
                log_file=log_file
            )
            key = f'chr{chrom}_cnv_count'
        feature_dict[key] = count
    return feature_dict

def ft_dn_ds(
        snpeff_vcf:str,
        bed_genes:str,
        bed_variants:str,
        bed_intersect:str,
        output_dir:str,
        feature_dict:dict[str:float|int],
        log_file:str,
        log_scr:str
    ) -> dict[str:float|int]:
    """
    Calculates the proportion of synonymous variants to nonsynonymous
    variants for all genes in ref_genome.
    """
    # Make the output directory if it does not exist
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        pass

    t = 'Starting to obtain Syn/Nonsyn variant proportion per gene...'
    log_print(
        t,
        level='info',
        log_file=log_file
    )
    # Define variants using environ
    os.environ['bed_variants'] = bed_variants
    os.environ['vcf_file'] = snpeff_vcf
    os.environ['bed_genes'] = bed_genes
    os.environ['bed_intersect'] = bed_intersect

    # Generate bed_variants file
    l = 'bcftools query -f \'%CHROM\t%POS\t%END\t%INFO/ANN\n\' $vcf_file'
    l += ' | awk \'BEGIN{OFS=\"\t\"} {print $1, $2-1, $2, $4}\''
    l += ' > $bed_variants'
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    log_print(
        f'{bed_variants} created.',
        level='info',
        log_file=log_file
    )
    # Intersect variants with genes
    l = f'bedtools intersect -a {bed_variants}'
    l += f' -b {bed_genes} -wa -wb > {bed_intersect}'
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    log_print(
        f'{bed_intersect} created.',
        level='info',
        log_file=log_file
    )

    # Define effect categories
    synonymous_terms = {"synonymous_variant"}
    nonsynonymous_terms = {
        "missense_variant",
        "stop_gained",
        "stop_lost",
        "start_lost",
        "protein_altering_variant",
        "inframe_insertion",
        "inframe_deletion",
        "frameshift_variant"
    }
    # Initialize defaultdict (dictionary) for counts
    gene_counts = defaultdict(
        lambda: {"synonymous": 0, "nonsynonymous": 0}
    )
    # Open the intersect bed file
    with open(bed_intersect, 'r') as f:
        # Go through lines
        for line in f:
            # Split line
            cols = line.strip().split("\t")
            # Get the gene and annotation fields
            ann_field = cols[3]
            gene = cols[7]
            # Parse the annotationfield
            effects = parse_ann_field(ann_field)
            # Go through obtained effects
            for effect in effects:
                if effect in synonymous_terms:
                    gene_counts[gene]["synonymous"] += 1
                    break # only count once per variant
                elif effect in nonsynonymous_terms:
                    gene_counts[gene]["nonsynonymous"] += 1
                    break # only count once per variant
    # Log and save results
    for gene, counts in gene_counts.items():
        syn = counts["synonymous"]
        nonsyn = counts["nonsynonymous"]
        ratio = syn / nonsyn if nonsyn > 0 else "Inf"
        key = f'{gene}_ratio_dN_dS'
        feature_dict[key] = ratio
    return feature_dict

def ft_fragment_lengths(
        sra_id:str,
        bam_file:str,
        output_dir:str,
        feature_dict:dict[str:float|int],
        log_file:str,
        log_scr:str
    ) -> dict[str:float|int]:
    """
    Calculates the mean, median and st. deviation of fragment lengths 
    for a BAM file.
    """
    # Make the output directory if it does not exist
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        pass
    # Define the fragment length file
    fl_file = os.path.join(output_dir, sra_id+'_fl.txt')
    # Initialize the scipt
    l = 'samtools view'
    # Define input bam file
    l += f' -f 0x2 {bam_file}'
    # Pipe to awk to select fragment length files
    l += ' | awk \'{if ($9>0 && $9<1000) print $9}\''
    # Define output
    l += f' > {fl_file}'
    # Run the script
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)

    # Open the created file to obtain values
    with open(fl_file, "r") as f:
        fragment_lengths = f.read().splitlines()
    # Convert to integers
    fragment_lengths = list(map(int, fragment_lengths))
    # Perform a first check for fragment lengths
    if len(fragment_lengths) == 0:
        w = 'No "properly paired" reads found.'
        w += ' Calculating read lengths'
        log_print(
            w,
            level='warn',
            log_file=log_file
        )
        # Rerun the script with less strict selection of paired ends
        l = f'samtools view -f 0x1 -F 0xC -F 0x904 {bam_file} | awk \''
        l += '{if ($9 != 0) {l=$9; if (l<0) l=-l; if (l<1000) print l}}\''
        l += f' > {fl_file}'
        log_code(
            l,
            log_file=log_scr
        )
        os.system(l)
        # Open fl_file again amd get fragment lengths
        with open(fl_file, "r") as f:
            fragment_lengths = f.read().splitlines()
        fragment_lengths = list(map(int, fragment_lengths))
    # Get mean, median and standard deviation of fragment lengths (fl)
    if len(fragment_lengths) > 0:
        feature_dict['fl_mean'] = statistics.mean(fragment_lengths)
        feature_dict['fl_median'] = statistics.median(fragment_lengths)
        feature_dict['fl_stdv'] = statistics.stdev(fragment_lengths)
        log_print(
            'Fragment mean, median and standard deviation:',
            level='info',
            log_file=log_file
        )
        w = str(feature_dict['fl_mean'])+'/'+ \
            str(feature_dict['fl_median'])+ \
            '/'+str(feature_dict['fl_stdv'])
        log_print(
            w,
            level='info',
            log_file=log_file
        )
    else:
        log_print(
            'No fragment lengths found. Returning NA.',
            level='warn',
            log_file=log_file
        )
        feature_dict['fl_mean'] = 'NA'
        feature_dict['fl_median'] = 'NA'
        feature_dict['fl_stdv'] = 'NA'
    
    return feature_dict

def ft_gv_per_region(
        vcf_file:str,
        gff_file:str,
        bed_file:str,
        counts_file:str,
        genome_sizes:str,
        bin_sizes:int,
        output_dir:str,
        feature_dict:dict[str:float|int],
        log_file:str,
        log_scr:str
    ) -> dict[str:float|int]:
    """
    Calculates the number of genetic variants (GVs) per region defined 
    in region_file.
    """
    # Make the output directory if it does not exist
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        pass

    # Obtain number of variants in genome bins
    bins_dict = variants_per_bin_os(
        vcf_file,
        genome_sizes,
        bin_sizes,
        log_file=log_file,
        log_scr=log_scr
    )
    # Load items from bins_dict to dict_features
    for key, value in bins_dict.items():
        feature_dict[key] = value

    # Count variants per region/gene in selected regions/genes
    log_print(
        'Starting to obtain variants per region and genes...',
        level='info',
        log_file=log_file
    )
    # Get regions from bed_file
    selected_regions = extract_regions(bed_file)
    all_genes = parse_gff_for_genes(gff_file)
    regions = selected_regions + all_genes
    # Create counts file
    create_counts(
        counts_file,
        regions,
        vcf_file,
        log_scr=log_scr
    )
    # Read counts back into Python variable
    counts = []
    with open(counts_file) as f:
        for line in f:
            name, count = line.strip().split("\t")
            counts.append((name, int(count)))
    # Save to dict_features
    for name, count in counts:
        key = f'{name}_variant_counts'
        feature_dict[key] = count
    return feature_dict

def parse_ann_field(ann_field:str) -> list[str]:
    """
    Parses the ANN field and returns a list of effects.
    """
    # Split the raw annotation field per effect
    annotations = ann_field.split(",")
    effects = []
    # Go through the different variant effects
    for ann in annotations:
        # Split into the different fields
        fields = ann.split("|")
        if len(fields) > 1:
            # Keep the effect field (0 is variant)
            effect = fields[1].strip()
            # Add to output list
            effects.append(effect)
    return effects

def parse_gff_for_genes(gff_file:str) -> list[list[str,int,int,str]]:
    """
    Parse a GFF3 file to extract gene regions without external libraries.

    Args:
        gff_file (str): Path to the GFF3 file.

    Returns:
        List of [contig, int(start), int(end), gene_name]
    """
    # Initialize the list that is returned
    regions = []

    with open(gff_file, 'r') as fh:
        for line in fh:
            # Skip comment lines
            if line.startswith("#"):
                continue
            fields = line.strip().split("\t")
            # Check for a valid GFF line
            if len(fields) != 9:
                continue
            # Assign fields
            seqid, source, feature_type, start, end, score, strand, phase, attributes = fields
            # Select only genes
            if feature_type.lower() != "gene":
                continue
            # Try to extract gene name from attributes
            attr_dict = {}
            for attr in attributes.strip().split(";"):
                if "=" in attr:
                    key, value = attr.split("=", 1)
                    attr_dict[key.strip()] = value.strip()

            gene_name = attr_dict.get("Name") or attr_dict.get("gene_name") or attr_dict.get("ID") or "unknown"

            regions.append([seqid, int(start), int(end), gene_name])

    return regions

def sam_to_bam(
        sra_id:str,
        sam_file:str,
        output_dir:str,
        log_scr:str
    ) -> str:
    """
    Transforms a SAM file into a BAM file.
    Returns the path to the output BAM file.
    """
    # Make the output directory if it does not exist
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        pass
    # Define output
    bam_file = os.path.join(output_dir, sra_id+'.bam')
    # Initialize script
    l = f'samtools view'
    # Define input file
    l += f' -bS {sam_file}'
    # Define output file
    l += f' -o {bam_file}'
    # Run the script
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    return bam_file

def sort_index_bam(
        sra_id:str,
        bam_file:str,
        output_dir:str,
        log_file:str,
        log_scr:str
    ) -> str:
    """
    Sorts and indexes a bam file using samtools sort and samtools index.
    Returns the sorted BAM file name.
    """
    # Make the output directory if it does not exist
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        pass
    # Define output
    bam_sorted = os.path.join(output_dir, sra_id+'_sorted.bam')
    log_print(
        f"Sorting BAM file: {bam_file} -> {bam_sorted}...",
        level='info',
        log_file=log_file
    )
    # Initialize the script to sort the bam file
    l = f'samtools sort {bam_file} -o {bam_sorted}'
    # Run the script
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    log_print(
        f"BAM sorting complete: {bam_sorted}",
        level='info',
        log_file=log_file
    )

    log_print(
        f"Indexing sorted BAM file: {bam_sorted}...",
        level='info',
        log_file=log_file
    )
    # Initialize the script to index the bam file
    l = f'samtools index {bam_sorted}'
    # Run the script
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    log_print(
        f"BAM indexing complete. Index file: {bam_sorted}.bai",
        level='info',
        log_file=log_file
    )
    
    return bam_sorted

def sra_id_data(sra_id:str, log_file:str) -> dict:
    """
    Retrieves SRA (Sequence Read Archive) metadata and download links
    from NCBI via the European Nucleotide Archive (ENA) API.

    Args:
        sra_id (str): The SRA accession ID (e.g., 'SRR000001', 
                      'ERR000001', 'DRR000001').

    Returns:
        dict | None: A dictionary containing SRA metadata and download 
                     links if successful, otherwise None.
    """
    # ENA's API endpoint for searching read runs.
    # We request JSON format and specify the fields we want to retrieve.
    # 'fastq_ftp' and 'sra_ftp' provide direct download links.
    # 'limit=1' ensures we only get one result for a specific accession.
    base_url = "https://www.ebi.ac.uk/ena/portal/api/search"
    f = "run_accession,fastq_ftp,sra_ftp,experiment_accession"
    f += ",sample_accession,study_accession,library_name,library_strategy"
    f += ",library_source,library_selection,instrument_platform"
    f += ",instrument_model,base_count,read_count,scientific_name,tax_id"
    params = {
        "result": "read_run",
        "query": f"run_accession={sra_id}",
        "fields": f,
        "format": "json",
        "limit": 1
    }
    w = f"Attempting to retrieve SRA data for: {sra_id}"
    log_print(
        w,
        level='info',
        log_file=log_file
    )
    try:
        # Make the HTTP GET request to the ENA API.
        response = requests.get(base_url, params=params)
        # Raise an HTTPError for bad responses (4xx or 5xx)
        response.raise_for_status()

        # Parse the JSON response.
        data = response.json()

        # The ENA API returns a list of results. For a single accession, 
        # it should be a list with one dictionary, or an empty list 
        # if not found.
        if data:
            sra_info = data[0]
            w = f"Successfully retrieved data for {sra_id}."
            log_print(
                w,
                level='info',
                log_file=log_file
            )
            return sra_info
        else:
            w = f"No SRA data found for accession ID: {sra_id}."
            w += ' Please check the ID.'
            log_print(
                w,
                level='info',
                log_file=log_file
            )
            return None

    except requests.exceptions.HTTPError as http_err:
        w = f"HTTP error occurred: {http_err}"
        w += f" - Status Code: {response.status_code}"
        log_print(
            w,
            level='info',
            log_file=log_file
        )
    except requests.exceptions.ConnectionError as conn_err:
        w = f"Connection error occurred: {conn_err}"
        w += " - Unable to connect to ENA API."
        log_print(
            w,
            level='info',
            log_file=log_file
        )
    except requests.exceptions.Timeout as timeout_err:
        w = f"Timeout error occurred: {timeout_err}"
        w += " - Request to ENA API timed out."
        log_print(
            w,
            level='info',
            log_file=log_file
        )
    except requests.exceptions.RequestException as req_err:
        w = f"An unexpected error occurred during the request: {req_err}"
        log_print(
            w,
            level='info',
            log_file=log_file
        )
    except json.JSONDecodeError as json_err:
        w = f"Error decoding JSON response: {json_err}."
        w += f" Response content: {response.text}"
        log_print(
            w,
            level='info',
            log_file=log_file
        )
    except Exception as e:
        log_print(
            f"An unexpected error occurred: {e}",
            level='warn',
            log_file=log_file
        )

    return None

def sra_to_fastq(
        sra_id:str,
        l_fastq:list[str],
        output_dir:str,
        log_file:str,
        log_scr:str,
        max_retry:int=5
    ) -> None:
    """
    Downloads a fastq file from a SRA ID using fastq-dump.
    """
    # Make the output directory if it does not exist
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        pass
    # Initialize script
    l = 'fastq-dump --gzip'
    if len(l_fastq)==2:
        l += ' --split-files'
    l += f' -O {output_dir} {sra_id}'
    # Start a run counter
    i = 0
    while i < max(1, max_retry):
        try:
            log_code(
                l,
                log_file=log_scr
            )
            os.system(l)
            # Exit loop if successful
            break
        except Exception as e:
            log_print(
                f"Attempt {i+1} failed: {e}",
                level='warn',
                log_file=log_file
            )
            # Check if the files were created and delete them if they exist
            for fq in l_fastq:
                if os.path.exists(fq):
                    os.remove(fq)
            # Increment the attempt counter
            i += 1
            if i >= max_retry:
                w = f"Failed to download FASTQ files after {i} attempts."
                log_print(
                    w,
                    level='error',
                    log_file=log_file
                )
                # This exits the loop
            else:
                # Wait before retrying
                time.sleep(60)
    return None

def variant_analysis_snpeff(
        sra_id:str,
        vcf_file:str,
        genome_name:str,
        snpeff_dir:str,
        output_dir:str,
        log_file:str,
        log_scr:str
    ) -> None:
    """
    Analyses a vcf file with snpeff. It compresses and indexes the output.
    """
    # Make the output directory if it does not exist
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        pass
    # Define output
    snpeff_vcf = os.path.join(output_dir, sra_id+'_snpeff.vcf')
    # Analyze variants in VCF with snpeff
    log_print(
        f"Analyzing variants from {vcf_file} with snpEff...",
        level='info',
        log_file=log_file
    )
    # Initialize script
    l = f'java -Xmx8g -jar {snpeff_dir}/snpEff/snpEff.jar'
    # Define snpeff genome name
    l += f' {genome_name}'
    # Define input vcf file
    l += f' {vcf_file}'
    # Define output vcf file
    l += f' > {snpeff_vcf}'
    # Run the script
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)

    # Visualize the snpeff vcf file
    l = f'tail {snpeff_vcf}'
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)

    # Run compression with bgzip
    log_print(
        f"Compressing {snpeff_vcf} with bgzip...",
        level='info',
        log_file=log_file
    )
    l = f'bgzip -c {snpeff_vcf} > {snpeff_vcf}.gz'
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    log_print(
        f"Compressed VCF: {snpeff_vcf}.gz",
        level='info',
        log_file=log_file
    )
    # Index the compressed snpeff vcf file
    log_print(
        f"Indexing {snpeff_vcf}.gz with tabix...",
        level='info',
        log_file=log_file
    )
    l = f'tabix -p vcf {snpeff_vcf}.gz'
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    log_print(
        f"VCF index: {snpeff_vcf}.gz.tbi",
        level='info',
        log_file=log_file
    )

    return snpeff_vcf+'.gz'

def variant_call_mpileup(
        sra_id:str,
        sorted_bam:str,
        ref_genome:str,
        output_dir:str,
        log_file:str,
        log_scr:str
    ) -> str:
    """
    Performs variant calling with bcftools mpileup.
    Returns the path to the output vcf file.
    """
    # Make the output directory if it does not exist
    try:
        os.mkdir(output_dir)
    except FileExistsError:
        pass
    # Define output files
    bcf_file = os.path.join(output_dir, sra_id+'.bcf')
    vcf_file = os.path.join(output_dir, sra_id+'.vcf')
    # Initialize script to generate bcf file
    l = 'bcftools mpileup'
    # Define reference genome
    l += f' -f {ref_genome}'
    # Define the sorted bam input file
    l += f' {sorted_bam}'
    # Define the output bcf file
    l += f' > {bcf_file}'
    # Run the script
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)

    log_print(
        f"Pileup and BCF file generated: {bcf_file}",
        level='info',
        log_file=log_file
    )
    # Check bcf file
    l = f'tail {bcf_file}'
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)

    # Initialize script to generate vcf file
    l = 'bcftools call -mv'
    # Define output file
    l += f' -o {vcf_file}'
    # Define input bcf file
    l += f' {bcf_file}'
    # Run the script
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)

    # Delete bcf file
    os.remove(bcf_file)

    log_print(
        f"Variant calling complete. Output VCF: {vcf_file}",
        level='info',
        log_file=log_file
    )
    # Check vcf file
    l = f'tail {vcf_file}'
    log_code(
        l,
        log_file=log_scr
    )
    os.system(l)
    return vcf_file

def variants_per_bin_os(
        vcf_file:str,
        genome_sizes:str,
        bin_size:int,
        log_file:str,
        log_scr:str
    ) -> dict[str, int]:
    """
    Count variants per genome bin using bedtools.

    Parameters:
        vcf_file (str): Path to a VCF file containing variant calls.
        genome_sizes (str): Path to a genome sizes file 
                            (e.g., UCSC .genome format).
        bin_size (int): Size of each genomic bin (window), in base pairs.

    Returns:
        dict: Dictionary with keys like 'variants_in_chr1:0-10000' 
              and values as counts.
    """
    # Initialize the output dictionary
    output_dict = {}
    # Create a temporary directory to store intermediate BED files
    with tempfile.TemporaryDirectory() as tmpdir:
        bins_bed = os.path.join(tmpdir, "genome_bins.bed")
        variants_bed = os.path.join(tmpdir, "variants.bed")
        intersected = os.path.join(tmpdir, "intersected.bed")
        # Generate genome bins using bedtools makewindows
        l = f"bedtools makewindows -g {genome_sizes} -w {bin_size}"
        l += f' > {bins_bed}'
        log_code(
            l,
            log_file=log_scr
        )
        if os.system(l) != 0:
            w = "Failed to generate genome bins using bedtools."
            log_print(
                w,
                level='warn',
                log_file=log_file
            )
            return {}
        # Convert VCF to BED format (skip headers and adjust coordinates)
        try:
            with gzip.open(vcf_file, 'rt') as vcf, \
                 open(variants_bed, "w") as bed:
                for line in vcf:
                    if line.startswith("#"):
                        continue
                    fields = line.strip().split("\t")
                    chrom = fields[0]
                    start = int(fields[1])-1 # BED is 0-based
                    ref = fields[3]
                    end = start + max(len(ref), 1) # Minimum 1 base
                    bed.write(f"{chrom}\t{start}\t{end}\n")
        except Exception as e:
            log_print(
                f"Failed to convert VCF to BED: {e}",
                level='warn',
                log_file=log_file
            )
            return {}
        # Count variants per bin using `bedtools intersect -c`
        l = f"bedtools intersect -a {bins_bed} -b {variants_bed} -c"
        l += f' > {intersected}'
        log_code(
            l,
            log_file=log_scr
        )
        if os.system(l) != 0:
            w = "Failed to intersect variants with genome bins."
            log_print(
                w,
                level='warn',
                log_file=log_file
            )
            return {}
        # Parse intersection output into a DataFrame
        try:
            df = pd.read_csv(
                intersected,
                sep="\t",
                header=None,
                names=["chrom", "start", "end", "variant_count"]
            )
        except Exception as e:
            log_print(
                f"Failed to read intersection result: {e}",
                level='warn',
                log_file=log_file
            )
            return {}
        # Format the output dictionary
        if df.empty:
            log_print(
                "No intersected variants found.",
                level='warn',
                log_file=log_file
            )
        else:
            df["bin_region"] = df.apply(
                lambda row: f"{row['chrom']}:{row['start']}-{row['end']}", 
                axis=1
                )
            for row in df.itertuples():
                key = f"variants_in_{row.bin_region}"
                output_dict[key] = row.variant_count
    return output_dict
